{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9166e4e7",
   "metadata": {},
   "source": [
    "## 2.1 - Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747c0138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9ff2e",
   "metadata": {},
   "source": [
    "## 2.2 - Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637705ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_missing(df):\n",
    "    missing = df.isnull().sum()\n",
    "    percent_missing = missing / data.shape[0] * 100\n",
    "    print(percent_missing);\n",
    "\n",
    "    \n",
    "def missing_heatmap(df):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.heatmap(df.isnull(), cmap='coolwarm')\n",
    "    plt.show();\n",
    "\n",
    "    \n",
    "def corrwith(df1, df2, column):\n",
    "    df1.corrwith(df2[column]).plot.bar(\n",
    "    figsize=(20,10), title=f'Level of correlation with {column}', grid=True\n",
    "    );\n",
    "\n",
    "    \n",
    "def heatmap(df):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    sns.heatmap(df.corr(), cmap='coolwarm', annot=True, linewidth=1);\n",
    "\n",
    "    \n",
    "def limits(column):\n",
    "    q1 = column.quantile(0.25)\n",
    "    q3 = column.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    return q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "\n",
    "\n",
    "def countplot(column, x_tick_rotation=False, x_rotation=90):\n",
    "    print(column.value_counts())\n",
    "    plt.figure(figsize=(15,5))\n",
    "    ax = sns.countplot(x=column)\n",
    "    if x_tick_rotation:\n",
    "        ax.tick_params(axis='x', rotation=x_rotation)\n",
    "\n",
    "\n",
    "def how_many_outliers(df, column):\n",
    "    inf_lim, sup_lim = limits(df[column])\n",
    "    inf = df.loc[df[column] < inf_lim].shape[0]\n",
    "    sup = df.loc[df[column] > sup_lim].shape[0]\n",
    "    total = inf + sup\n",
    "    return inf, sup, total\n",
    "    \n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    n_of_rows = df.shape[0]\n",
    "    inf_lim, sup_lim = limits(df[column])\n",
    "    df = df.loc[(df[column] >= inf_lim) & (df[column] <= sup_lim), :]\n",
    "    removed_rows = n_of_rows - df.shape[0]\n",
    "    return df, removed_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d667579d",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "908c4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data = pd.read_csv(r'C:\\Users\\Henrique\\Desktop\\0Machine Learning & Deep Learning Projects for Beginners (TutsNode.net) 2023\\02 - Project 1 Breast Cancer Detection/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220f153",
   "metadata": {},
   "source": [
    "### Here we are going to delete the 'Unnamed: 32' column that has all null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67555ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Unnamed: 32', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ea0da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba9d8a",
   "metadata": {},
   "source": [
    "### 'id'\n",
    "- Let's eliminate the 'id' column. It will be irrelevant for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48826c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('id', axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62349691",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "- Now let's convert categorical data into a numerical format that can be used by our machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20222930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B    357\n",
      "M    212\n",
      "Name: diagnosis, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "countplot(data['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a31fc782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    357\n",
       "M    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346c800",
   "metadata": {},
   "source": [
    "### Performing encoding with 'pd.get_dummies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b1ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data=data, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb6b9b",
   "metadata": {},
   "source": [
    "Now our target is:\n",
    "\n",
    "- 0 correponds to B (benign)\n",
    "- 1 corresponds to M (malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857c2060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    357\n",
      "1    212\n",
      "Name: diagnosis_M, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "countplot(data['diagnosis_M'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51079b90",
   "metadata": {},
   "source": [
    "### Train x Test\n",
    "- Splitting the data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "306d1a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('diagnosis_M', axis=1)\n",
    "y = data['diagnosis_M']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4121252c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea42918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab4b03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6534f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544c902",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "- Let's normalize the features of our dataset so that they are on the same scale, improving the performance of the ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed3834f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e15d6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd8d22b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74998027, -1.09978744, -0.74158608, ..., -0.6235968 ,\n",
       "         0.07754241,  0.45062841],\n",
       "       [-1.02821446, -0.1392617 , -1.02980434, ..., -0.7612376 ,\n",
       "        -1.07145262, -0.29541379],\n",
       "       [-0.53852228, -0.29934933, -0.56857428, ..., -0.50470441,\n",
       "         0.34900827, -0.13371556],\n",
       "       ...,\n",
       "       [-1.3214733 , -0.20855336, -1.3143845 , ..., -0.98621857,\n",
       "        -0.69108476, -0.13148524],\n",
       "       [-1.24245479, -0.23244704, -1.27759928, ..., -1.7562754 ,\n",
       "        -1.55125275, -1.01078909],\n",
       "       [-0.74441558,  1.13188181, -0.72016173, ..., -0.28490593,\n",
       "        -1.2308599 ,  0.20083251]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78d7c7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21855296,  0.31710749, -0.14938447, ...,  1.36998461,\n",
       "         1.0939611 ,  1.51560644],\n",
       "       [-0.27141746,  1.44727832, -0.33290634, ..., -0.84095647,\n",
       "        -0.70686766, -0.88310324],\n",
       "       [-0.04604776, -0.84412512, -0.11098232, ..., -0.50394228,\n",
       "        -1.19298094, -0.92659449],\n",
       "       ...,\n",
       "       [-0.13230036, -0.12253613, -0.14574637, ...,  0.20087616,\n",
       "        -0.06766026,  0.36754897],\n",
       "       [-0.24637638,  0.56799108, -0.28682376, ..., -0.90055508,\n",
       "        -0.40225771, -0.95558866],\n",
       "       [-1.35013142,  0.65639767, -1.34712739, ..., -1.36545479,\n",
       "        -0.72107227, -0.54576727]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2164c60",
   "metadata": {},
   "source": [
    "### PCA - Principal Component Analysis\n",
    "- The use of PCA (Principal Component Analysis) in this project is justified by the need to reduce the dimensionality of the data set.' With 30 initial features, applying PCA allows condensing this information into a smaller number of principal components while preserving most of the data's variance. This not only simplifies data processing and analysis but also helps to avoid multicollinearity issues, making the classification model more robust. Additionally, PCA can reveal which features are most important in discriminating between benign and malignant classes, contributing to the predictive model's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f0e29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=15\n",
    "pca = PCA(n_components=n_components) \n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8348ae57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plote a variação explicada acumulada em relação ao número de componentes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Accumulated Explained Variation')\n",
    "plt.title('Principal Component Analysis')\n",
    "plt.grid(True)\n",
    "for i, explained_var in enumerate(cumulative_explained_variance):\n",
    "    plt.annotate(f'{explained_var:.2f}', (i + 1, explained_var), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f7883",
   "metadata": {},
   "source": [
    "# 4. **Modeling**:\n",
    "\n",
    "- In the \"Modeling\" phase of CRISP-DM, the main focus is to build machine learning models to solve the problem.\n",
    "- This phase involves selecting the modeling algorithm, training, and evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2b37a",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f78c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"Model\": model,\n",
    "        \"Accuracy Score\": accuracy,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Precision Score\": precision,\n",
    "        \"Recall Score\": recall\n",
    "    }\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4ce8f",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d744ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(columns=['Model', 'Accuracy Score', 'F1 Score', 'Precision Score', 'Recall Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e430bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_rf = RandomForestClassifier()\n",
    "model_ab = AdaBoostClassifier()\n",
    "model_knn = KNeighborsClassifier()\n",
    "model_nb = GaussianNB()\n",
    "\n",
    "models = {'Logistic Regression': model_lr,\n",
    "         'Random Forest': model_rf,\n",
    "          'AdaBoost': model_ab,\n",
    "          'KNN': model_knn,\n",
    "          'Naive Bayes':model_nb\n",
    "         }\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Get the actual model from the dictionary\n",
    "    ml_model = models[model_name]\n",
    "    \n",
    "    ml_model.fit(X_train, y_train)\n",
    "    y_pred = ml_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics_dict = evaluate_model(model_name, y_test, y_pred)\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    metrics = pd.concat([metrics, pd.DataFrame([metrics_dict])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872cb6f1",
   "metadata": {},
   "source": [
    "Let's arrange them in descending order by R² Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ce89b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>Recall Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.97076</td>\n",
       "      <td>0.96124</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.968254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Accuracy Score  F1 Score Precision Score Recall Score\n",
       "0  Logistic Regression       0.976608  0.967742        0.983607     0.952381\n",
       "1        Random Forest        0.97076   0.96124        0.939394     0.984127\n",
       "2             AdaBoost       0.964912  0.953125        0.938462     0.968254\n",
       "3                  KNN       0.959064  0.942149        0.982759     0.904762\n",
       "4          Naive Bayes       0.912281  0.883721        0.863636     0.904762"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.sort_values(by=\"Accuracy Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce2dd8",
   "metadata": {},
   "source": [
    "# 5. **Evaluation**:\n",
    "\n",
    "- In the \"Evaluation\" phase of CRISP-DM, the goal is to assess the performance of the model built in the previous phase.\n",
    "- This involves model validation using appropriate metrics, interpreting the results, and verifying if the model meets the established success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ecac5b",
   "metadata": {},
   "source": [
    "- Let's check our models before the parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391a4e4",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45671f",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "038805c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regressor_tuning(X, y):\n",
    "    # Kfold\n",
    "    kfold = KFold(n_splits=5)\n",
    "    \n",
    "    # GridSearchCV parameters\n",
    "    penalty = ['l1', 'l2', 'elasticnet', None]\n",
    "    tol = np.array([0.00001, 0.0001, 0.001, 0.01, 0,1])\n",
    "    C = np.array([0.8, 0.9, 1.0, 1.1, 1.2])\n",
    "    solver = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "    max_iter = np.array([90, 100, 110])\n",
    "    warm_start = [True, False]\n",
    "    param_grid = {'penalty':penalty, 'tol':tol, 'C':C, 'solver':solver, 'max_iter':max_iter, 'warm_start':warm_start}\n",
    "    \n",
    "    # ML Model\n",
    "    logistic = LogisticRegression()\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_logistic = GridSearchCV(logistic, param_grid=param_grid, cv=kfold, n_jobs=3)\n",
    "    grid_logistic.fit(X, y)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = grid_logistic.predict(X)\n",
    "    \n",
    "    # Metrics calculation\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    confusion = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Result\n",
    "    print('BEST RESULTS:')\n",
    "    print(f'Best score: {grid_logistic.best_score_}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Precision Score: {precision}')\n",
    "    print(f'Recall Score: {recall}')\n",
    "    print(f'Best estimators: {grid_logistic.best_estimator_}')\n",
    "    print(f'Confusion Matrix: \\n{confusion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14f6b251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST RESULTS:\n",
      "Best score: 0.9875\n",
      "Accuracy: 0.9899497487437185\n",
      "F1 Score: 0.9863945578231292\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.9731543624161074\n",
      "Best estimators: LogisticRegression(C=0.8, max_iter=90, tol=1e-05, warm_start=True)\n",
      "Confusion Matrix: \n",
      "[[249   0]\n",
      " [  4 145]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15600 fits failed out of a total of 21600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1528, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1186, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'eps <= 0'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 434, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only solvers in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], got newton-cholesky.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4500 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor_tuning(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e438d",
   "metadata": {},
   "source": [
    "# 6. **Deploy**:\n",
    "\n",
    "- In the \"Deploy\" phase of CRISP-DM, the focus is on presenting the results of the Data Science project and preparing for the implementation of the model in a production environment.\n",
    "- This phase can involve creating reports, visualizations, and documenting the project.\n",
    "- I will import the necessary libraries again (in case the deployment file is opened in a separate file)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4608e62",
   "metadata": {},
   "source": [
    "import tkinter as tk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Função para fazer a predição com LogisticRegression\n",
    "def breast_cancer_predict():\n",
    "    # Coleta os dados dos campos\n",
    "    try:\n",
    "        radius_m = float(radius_mean.get())\n",
    "        texture_m = float(texture_mean.get())\n",
    "        perimeter_m = float(perimeter_mean.get())\n",
    "        area_m = float(area_mean.get())\n",
    "        smoothness_m = float(smoothness_mean.get())\n",
    "        compactness_m = float(compactness_mean.get())\n",
    "        concavity_m = float(concavity_mean.get())\n",
    "        concave_points_m = float(concave_points_mean.get())\n",
    "        symmetry_m = float(symmetry_mean.get())\n",
    "        fractal_dimension_m = float(fractal_dimension_mean.get())\n",
    "        radius_s = float(radius_se.get())\n",
    "        texture_s = float(texture_se.get())\n",
    "        perimeter_s = float(perimeter_se.get())\n",
    "        area_s = float(area_se.get())\n",
    "        smoothness_s = float(smoothness_se.get())\n",
    "        compactness_s = float(compactness_se.get())\n",
    "        concavity_s = float(concavity_se.get())\n",
    "        concave_points_s = float(concave_points_se.get())\n",
    "        symmetry_s = float(symmetry_se.get())\n",
    "        fractal_dimension_s = float(fractal_dimension_se.get())\n",
    "        radius_w = float(radius_worst.get())\n",
    "        texture_w = float(texture_worst.get())\n",
    "        perimeter_w = float(perimeter_worst.get())\n",
    "        area_w = float(area_worst.get())\n",
    "        smoothness_w = float(smoothness_worst.get())\n",
    "        compactness_w = float(compactness_worst.get())\n",
    "        concavity_w = float(concavity_worst.get())\n",
    "        concave_points_w = float(concave_points_worst.get())\n",
    "        symmetry_w = float(symmetry_worst.get())\n",
    "        fractal_dimension_w = float(fractal_dimension_worst.get())      \n",
    "\n",
    "        # Crie um array numpy com os dados para a predição\n",
    "        data = np.array([[radius_m, texture_m, perimeter_m, area_m, smoothness_m, compactness_m, concavity_m, \n",
    "                          concave_points_m, symmetry_m, fractal_dimension_m, radius_s, texture_s, perimeter_s, area_s,\n",
    "                          smoothness_s, compactness_s, concavity_s, concave_points_s, symmetry_s, fractal_dimension_s, radius_w, texture_w,\n",
    "                          perimeter_w, area_w, smoothness_w, compactness_w, concavity_w, concave_points_w, symmetry_w, fractal_dimension_w]])\n",
    "\n",
    "        # Crie um modelo de LogisticRegression fictício (substitua pelo seu modelo real)\n",
    "        model = LogisticRegression(C=1.2, max_iter=90, solver='liblinear', tol=1.0, warm_start=True)\n",
    "\n",
    "        # Treine o modelo (substitua pelo treinamento do seu modelo real)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Faça a predição\n",
    "        prediction = model.predict(data)\n",
    "\n",
    "        # Traduza a predição em 'malignant' ou 'benign'\n",
    "        if prediction[0] == 1:\n",
    "            result_text = 'Malignant'\n",
    "            result_bg_color = 'red'\n",
    "        else:\n",
    "            result_text = 'Benign'\n",
    "            result_bg_color = 'green'\n",
    "\n",
    "        # Atualize o texto do label_button com o resultado da predição\n",
    "        font_style_predict = (\"Arial\", 14)\n",
    "        label_button.config(text=f'{result_text}', relief='solid', font=font_style_predict, fg='white')\n",
    "        label_button.configure(bg=result_bg_color)\n",
    "        \n",
    "    except ValueError:\n",
    "        # Tratamento de erro para campos vazios ou não numéricos\n",
    "        label_button.config(text='Error: Fill in all fields with numerical values', relief='solid', bg='red')\n",
    "\n",
    "program = tk.Tk()\n",
    "\n",
    "program.title('Breast Cancer Diagnostic')\n",
    "\n",
    "# Main label\n",
    "font_style = (\"Arial\", 20)\n",
    "label_main = tk.Label(text='PYHTON CANCER INSTITUTE\\nBreast Cancer Diagnostic System', borderwidth=1, relief='solid', font=font_style, bg='#658bd2', fg='white')\n",
    "label_main.grid(row=0, column=0, padx=10, pady=10,sticky='nswe', columnspan=10)\n",
    "\n",
    "# Columns - first row\n",
    "label_radius_mean = tk.Label(text='radius_mean')\n",
    "label_radius_mean.grid(row=1, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "radius_mean = tk.Entry()\n",
    "radius_mean.grid(row=1, column=1)\n",
    "\n",
    "label_texture_mean = tk.Label(text='texture_mean')\n",
    "label_texture_mean.grid(row=1, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "texture_mean = tk.Entry()\n",
    "texture_mean.grid(row=1, column=3)\n",
    "\n",
    "label_perimeter_mean = tk.Label(text='perimeter_mean')\n",
    "label_perimeter_mean.grid(row=1, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "perimeter_mean = tk.Entry()\n",
    "perimeter_mean.grid(row=1, column=5)\n",
    "\n",
    "label_area_mean = tk.Label(text='area_mean')\n",
    "label_area_mean.grid(row=1, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "area_mean = tk.Entry()\n",
    "area_mean.grid(row=1, column=7)\n",
    "\n",
    "label_smoothness_mean = tk.Label(text='smoothness_mean')\n",
    "label_smoothness_mean.grid(row=1, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "smoothness_mean = tk.Entry()\n",
    "smoothness_mean.grid(row=1, column=9)\n",
    "\n",
    "\n",
    "# Columns - second row\n",
    "label_compactness_mean = tk.Label(text='compactness_mean')\n",
    "label_compactness_mean.grid(row=2, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "compactness_mean = tk.Entry()\n",
    "compactness_mean.grid(row=2, column=1)\n",
    "\n",
    "label_concavity_mean = tk.Label(text='concavity_mean')\n",
    "label_concavity_mean.grid(row=2, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concavity_mean = tk.Entry()\n",
    "concavity_mean.grid(row=2, column=3)\n",
    "\n",
    "label_concave_points_mean = tk.Label(text='concave points_mean')\n",
    "label_concave_points_mean.grid(row=2, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concave_points_mean = tk.Entry()\n",
    "concave_points_mean.grid(row=2, column=5)\n",
    "\n",
    "label_symmetry_mean = tk.Label(text='symmetry_mean')\n",
    "label_symmetry_mean.grid(row=2, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "symmetry_mean = tk.Entry()\n",
    "symmetry_mean.grid(row=2, column=7)\n",
    "\n",
    "label_fractal_dimension_mean = tk.Label(text='fractal_dimension_mean')\n",
    "label_fractal_dimension_mean.grid(row=2, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "fractal_dimension_mean = tk.Entry()\n",
    "fractal_dimension_mean.grid(row=2, column=9)\n",
    "\n",
    "\n",
    "# Columns - third row\n",
    "label_radius_se = tk.Label(text='radius_se')\n",
    "label_radius_se.grid(row=3, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "radius_se = tk.Entry()\n",
    "radius_se.grid(row=3, column=1)\n",
    "\n",
    "label_texture_se = tk.Label(text='texture_se')\n",
    "label_texture_se.grid(row=3, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "texture_se = tk.Entry()\n",
    "texture_se.grid(row=3, column=3)\n",
    "\n",
    "label_perimeter_se = tk.Label(text='perimeter_se')\n",
    "label_perimeter_se.grid(row=3, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "perimeter_se = tk.Entry()\n",
    "perimeter_se.grid(row=3, column=5)\n",
    "\n",
    "label_area_se = tk.Label(text='area_se')\n",
    "label_area_se.grid(row=3, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "area_se = tk.Entry()\n",
    "area_se.grid(row=3, column=7)\n",
    "\n",
    "label_smoothness_se = tk.Label(text='smoothness_se')\n",
    "label_smoothness_se.grid(row=3, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "smoothness_se = tk.Entry()\n",
    "smoothness_se.grid(row=3, column=9)\n",
    "\n",
    "\n",
    "# Columns - fourth row\n",
    "label_compactness_se = tk.Label(text='compactness_se')\n",
    "label_compactness_se.grid(row=4, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "compactness_se = tk.Entry()\n",
    "compactness_se.grid(row=4, column=1)\n",
    "\n",
    "label_concavity_se = tk.Label(text='concavity_se')\n",
    "label_concavity_se.grid(row=4, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concavity_se = tk.Entry()\n",
    "concavity_se.grid(row=4, column=3)\n",
    "\n",
    "label_concave_points_se = tk.Label(text='concave points_se')\n",
    "label_concave_points_se.grid(row=4, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concave_points_se = tk.Entry()\n",
    "concave_points_se.grid(row=4, column=5)\n",
    "\n",
    "label_symmetry_se = tk.Label(text='symmetry_se')\n",
    "label_symmetry_se.grid(row=4, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "symmetry_se = tk.Entry()\n",
    "symmetry_se.grid(row=4, column=7)\n",
    "\n",
    "label_fractal_dimension_se = tk.Label(text='fractal_dimension_se')\n",
    "label_fractal_dimension_se.grid(row=4, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "fractal_dimension_se = tk.Entry()\n",
    "fractal_dimension_se.grid(row=4, column=9)\n",
    "\n",
    "\n",
    "# Columns - fifth row\n",
    "label_radius_worst = tk.Label(text='radius_worst')\n",
    "label_radius_worst.grid(row=5, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "radius_worst = tk.Entry()\n",
    "radius_worst.grid(row=5, column=1)\n",
    "\n",
    "label_texture_worst = tk.Label(text='texture_worst')\n",
    "label_texture_worst.grid(row=5, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "texture_worst = tk.Entry()\n",
    "texture_worst.grid(row=5, column=3)\n",
    "\n",
    "label_perimeter_worst = tk.Label(text='perimeter_worst')\n",
    "label_perimeter_worst.grid(row=5, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "perimeter_worst = tk.Entry()\n",
    "perimeter_worst.grid(row=5, column=5)\n",
    "\n",
    "label_area_worst = tk.Label(text='area_worst')\n",
    "label_area_worst.grid(row=5, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "area_worst = tk.Entry()\n",
    "area_worst.grid(row=5, column=7)\n",
    "\n",
    "label_smoothness_worst = tk.Label(text='smoothness_worst')\n",
    "label_smoothness_worst.grid(row=5, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "smoothness_worst = tk.Entry()\n",
    "smoothness_worst.grid(row=5, column=9)\n",
    "\n",
    "\n",
    "# Columns - sixth row\n",
    "label_compactness_worst = tk.Label(text='compactness_worst')\n",
    "label_compactness_worst.grid(row=6, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "compactness_worst = tk.Entry()\n",
    "compactness_worst.grid(row=6, column=1)\n",
    "\n",
    "label_concavity_worst = tk.Label(text='concavity_worst')\n",
    "label_concavity_worst.grid(row=6, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concavity_worst = tk.Entry()\n",
    "concavity_worst.grid(row=6, column=3)\n",
    "\n",
    "label_concave_points_worst = tk.Label(text='concave points_worst')\n",
    "label_concave_points_worst.grid(row=6, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concave_points_worst = tk.Entry()\n",
    "concave_points_worst.grid(row=6, column=5)\n",
    "\n",
    "label_symmetry_worst = tk.Label(text='symmetry_worst')\n",
    "label_symmetry_worst.grid(row=6, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "symmetry_worst = tk.Entry()\n",
    "symmetry_worst.grid(row=6, column=7)\n",
    "\n",
    "label_fractal_dimension_worst = tk.Label(text='fractal_dimension_worst')\n",
    "label_fractal_dimension_worst.grid(row=6, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "fractal_dimension_worst = tk.Entry()\n",
    "fractal_dimension_worst.grid(row=6, column=9)\n",
    "\n",
    "# Button\n",
    "font_style_button = (\"Arial\", 14)\n",
    "button_predict = tk.Button(text='Perform analysis', command=breast_cancer_predict, bg='#658bd2', font=font_style_button, fg='white')\n",
    "button_predict.grid(row=7, column=3, padx=10, pady=10, sticky='nswe', columnspan=2)\n",
    "\n",
    "# Show result\n",
    "label_button = tk.Label(text='Result: N/A', borderwidth=1, relief='solid', font=font_style_button)\n",
    "label_button.grid(row=7, column=5, padx=10, pady=10, sticky='nswe', columnspan=2)\n",
    "\n",
    "program.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d0214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fc1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef4e82c5",
   "metadata": {},
   "source": [
    "# Está FUNCIONANDO\n",
    "# Vamos testar algumas amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7146040",
   "metadata": {},
   "outputs": [],
   "source": [
    "benigno=\"13.540\t14.36\t87.46\t566.3\t0.09779\t0.08129\t0.066640\t0.047810\t0.1885\t0.05766\t0.2699\t0.7886\t2.058\t23.560\t0.008462\t0.014600\t0.023870\t0.013150\t0.01980\t0.002300\t15.110\t19.26\t99.70\t711.2\t0.14400\t0.17730\t0.239000\t0.12880\t0.2977\t0.07259\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf998b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "maligno='12.450\t15.70\t82.57\t477.1\t0.12780\t0.17000\t0.157800\t0.080890\t0.2087\t0.07613\t0.3345\t0.8902\t2.217\t27.190\t0.007510\t0.033450\t0.036720\t0.011370\t0.02165\t0.005082\t15.470\t23.75\t103.40\t741.6\t0.17910\t0.52490\t0.535500\t0.17410\t0.3985\t0.12440'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c81286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benigno63='9.173\t13.86\t59.20\t260.9\t0.07721\t0.08751\t0.059880\t0.021800\t0.2341\t0.06963\t0.4098\t2.2650\t2.608\t23.520\t0.008738\t0.039380\t0.043120\t0.015600\t0.04192\t0.005822\t10.010\t19.23\t65.59\t310.1\t0.09836\t0.16780\t0.139700\t0.05087\t0.3282\t0.08490\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4226639",
   "metadata": {},
   "outputs": [],
   "source": [
    "maligno62='14.250\t22.15\t96.42\t645.7\t0.10490\t0.20080\t0.213500\t0.086530\t0.1949\t0.07292\t0.7036\t1.2680\t5.373\t60.780\t0.009407\t0.070560\t0.068990\t0.018480\t0.01700\t0.006113\t17.670\t29.51\t119.10\t959.5\t0.16400\t0.62470\t0.692200\t0.17850\t0.2844\t0.11320\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0c43920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substituir_tab_por_virgula(dados):\n",
    "    dados_formatados = dados.replace('\\t', ',')\n",
    "    return dados_formatados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1394fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.250,22.15,96.42,645.7,0.10490,0.20080,0.213500,0.086530,0.1949,0.07292,0.7036,1.2680,5.373,60.780,0.009407,0.070560,0.068990,0.018480,0.01700,0.006113,17.670,29.51,119.10,959.5,0.16400,0.62470,0.692200,0.17850,0.2844,0.11320,\n"
     ]
    }
   ],
   "source": [
    "dados_formatados = substituir_tab_por_virgula(maligno62)\n",
    "print(dados_formatados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ceffe017",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[14.250,22.15,96.42,645.7,0.10490,0.20080,0.213500,0.086530,0.1949,0.07292,0.7036,1.2680,5.373,60.780,0.009407,0.070560,0.068990,0.018480,0.01700,0.006113,17.670,29.51,119.10,959.5,0.16400,0.62470,0.692200,0.17850,0.2844,0.11320]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3828d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predição: 1 (Maligno)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Crie um modelo de LogisticRegression fictício (substitua pelo seu modelo real)\n",
    "model = LogisticRegression(C=1.75, random_state=0, solver='sag')\n",
    "\n",
    "# Treine o modelo (substitua pelo treinamento do seu modelo real)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Crie uma instância do StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Ajuste o scaler aos seus dados de treinamento\n",
    "#scaler.fit(X_train)\n",
    "\n",
    "# Aplique o scaler aos dados de entrada do usuário\n",
    "#data_scaled = scaler.transform(data)\n",
    "\n",
    "# Faça a predição\n",
    "prediction = model.predict(scaler.transform(data))\n",
    "\n",
    "if prediction[0] == 1:\n",
    "    resultado_predicao = \"Predição: 1 (Maligno)\"\n",
    "else:\n",
    "    resultado_predicao = \"Predição: 0 (Benigno)\"\n",
    "\n",
    "# Imprima o resultado da predição\n",
    "print(resultado_predicao)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0297561f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>diagnosis_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.080</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.230</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.074580</td>\n",
       "      <td>0.056610</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.686900</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>0.3345</td>\n",
       "      <td>0.8902</td>\n",
       "      <td>2.217</td>\n",
       "      <td>27.190</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.033450</td>\n",
       "      <td>0.036720</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.02165</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>0.4467</td>\n",
       "      <td>0.7732</td>\n",
       "      <td>3.180</td>\n",
       "      <td>53.910</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>0.022540</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.01369</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>22.880</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>0.5835</td>\n",
       "      <td>1.3770</td>\n",
       "      <td>3.856</td>\n",
       "      <td>50.960</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.030290</td>\n",
       "      <td>0.024880</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.01486</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>1.0020</td>\n",
       "      <td>2.406</td>\n",
       "      <td>24.320</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>0.035020</td>\n",
       "      <td>0.035530</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>0.02143</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>15.490</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>0.2976</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>2.039</td>\n",
       "      <td>23.940</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>0.072170</td>\n",
       "      <td>0.077430</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.105000</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>0.3795</td>\n",
       "      <td>1.1870</td>\n",
       "      <td>2.466</td>\n",
       "      <td>40.510</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.01460</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>19.190</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>0.5058</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>3.564</td>\n",
       "      <td>54.160</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.040610</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>20.420</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.396500</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>1.0780</td>\n",
       "      <td>2.903</td>\n",
       "      <td>36.580</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>0.031260</td>\n",
       "      <td>0.050510</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.02981</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>16.840</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>0.2121</td>\n",
       "      <td>1.1690</td>\n",
       "      <td>2.061</td>\n",
       "      <td>19.210</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>0.059360</td>\n",
       "      <td>0.055010</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.01961</td>\n",
       "      <td>0.008093</td>\n",
       "      <td>15.030</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>1.0330</td>\n",
       "      <td>2.879</td>\n",
       "      <td>32.550</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.047410</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.01857</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>17.460</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.702600</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>0.4727</td>\n",
       "      <td>1.2400</td>\n",
       "      <td>3.195</td>\n",
       "      <td>45.400</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>19.070</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.291400</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>1.0730</td>\n",
       "      <td>3.854</td>\n",
       "      <td>54.180</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.025010</td>\n",
       "      <td>0.031880</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>0.01689</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>20.960</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.478400</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>0.2699</td>\n",
       "      <td>0.7886</td>\n",
       "      <td>2.058</td>\n",
       "      <td>23.560</td>\n",
       "      <td>0.008462</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.023870</td>\n",
       "      <td>0.013150</td>\n",
       "      <td>0.01980</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>0.7477</td>\n",
       "      <td>1.383</td>\n",
       "      <td>14.670</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>0.016980</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.01678</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.2773</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>1.909</td>\n",
       "      <td>15.700</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.02027</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.088670</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>0.4388</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>3.384</td>\n",
       "      <td>44.910</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.053280</td>\n",
       "      <td>0.064460</td>\n",
       "      <td>0.022520</td>\n",
       "      <td>0.03672</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>18.070</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.630500</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>0.2545</td>\n",
       "      <td>0.9832</td>\n",
       "      <td>2.110</td>\n",
       "      <td>21.050</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.030550</td>\n",
       "      <td>0.026810</td>\n",
       "      <td>0.013520</td>\n",
       "      <td>0.01454</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>17.620</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>1.0120</td>\n",
       "      <td>3.498</td>\n",
       "      <td>43.500</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>0.035760</td>\n",
       "      <td>0.010830</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>20.270</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>0.6003</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>4.655</td>\n",
       "      <td>61.100</td>\n",
       "      <td>0.005627</td>\n",
       "      <td>0.030330</td>\n",
       "      <td>0.034070</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.01925</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>20.010</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11.840</td>\n",
       "      <td>18.70</td>\n",
       "      <td>77.93</td>\n",
       "      <td>440.6</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>0.15160</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.051820</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.07799</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>1.0300</td>\n",
       "      <td>3.475</td>\n",
       "      <td>41.000</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.02273</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>16.820</td>\n",
       "      <td>28.12</td>\n",
       "      <td>119.40</td>\n",
       "      <td>888.7</td>\n",
       "      <td>0.16370</td>\n",
       "      <td>0.57750</td>\n",
       "      <td>0.695600</td>\n",
       "      <td>0.15460</td>\n",
       "      <td>0.4761</td>\n",
       "      <td>0.14020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>17.020</td>\n",
       "      <td>23.98</td>\n",
       "      <td>112.80</td>\n",
       "      <td>899.3</td>\n",
       "      <td>0.11970</td>\n",
       "      <td>0.14960</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.2248</td>\n",
       "      <td>0.06382</td>\n",
       "      <td>0.6009</td>\n",
       "      <td>1.3980</td>\n",
       "      <td>3.999</td>\n",
       "      <td>67.780</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.02102</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>20.880</td>\n",
       "      <td>32.09</td>\n",
       "      <td>136.10</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>0.16340</td>\n",
       "      <td>0.35590</td>\n",
       "      <td>0.558800</td>\n",
       "      <td>0.18470</td>\n",
       "      <td>0.3530</td>\n",
       "      <td>0.08482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>19.270</td>\n",
       "      <td>26.47</td>\n",
       "      <td>127.90</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>0.09401</td>\n",
       "      <td>0.17190</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.075930</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.06261</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.6062</td>\n",
       "      <td>3.528</td>\n",
       "      <td>68.170</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>0.033180</td>\n",
       "      <td>0.034970</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.01543</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>24.150</td>\n",
       "      <td>30.90</td>\n",
       "      <td>161.40</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>0.15090</td>\n",
       "      <td>0.65900</td>\n",
       "      <td>0.609100</td>\n",
       "      <td>0.17850</td>\n",
       "      <td>0.3672</td>\n",
       "      <td>0.11230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16.130</td>\n",
       "      <td>17.88</td>\n",
       "      <td>107.00</td>\n",
       "      <td>807.2</td>\n",
       "      <td>0.10400</td>\n",
       "      <td>0.15590</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.077520</td>\n",
       "      <td>0.1998</td>\n",
       "      <td>0.06515</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>2.183</td>\n",
       "      <td>35.030</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.028680</td>\n",
       "      <td>0.026640</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>0.01703</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>20.210</td>\n",
       "      <td>27.26</td>\n",
       "      <td>132.70</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.14460</td>\n",
       "      <td>0.58040</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>0.18640</td>\n",
       "      <td>0.4270</td>\n",
       "      <td>0.12330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16.740</td>\n",
       "      <td>21.59</td>\n",
       "      <td>110.10</td>\n",
       "      <td>869.5</td>\n",
       "      <td>0.09610</td>\n",
       "      <td>0.13360</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>0.060180</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.05656</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.9197</td>\n",
       "      <td>3.008</td>\n",
       "      <td>45.190</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.024990</td>\n",
       "      <td>0.036950</td>\n",
       "      <td>0.011950</td>\n",
       "      <td>0.02789</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>20.010</td>\n",
       "      <td>29.02</td>\n",
       "      <td>133.50</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>0.15630</td>\n",
       "      <td>0.38350</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.18130</td>\n",
       "      <td>0.4863</td>\n",
       "      <td>0.08633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>14.250</td>\n",
       "      <td>21.72</td>\n",
       "      <td>93.63</td>\n",
       "      <td>633.0</td>\n",
       "      <td>0.09823</td>\n",
       "      <td>0.10980</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>2.657</td>\n",
       "      <td>24.910</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.029950</td>\n",
       "      <td>0.048150</td>\n",
       "      <td>0.011610</td>\n",
       "      <td>0.02028</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>15.890</td>\n",
       "      <td>30.36</td>\n",
       "      <td>116.20</td>\n",
       "      <td>799.6</td>\n",
       "      <td>0.14460</td>\n",
       "      <td>0.42380</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.14470</td>\n",
       "      <td>0.3591</td>\n",
       "      <td>0.10140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13.030</td>\n",
       "      <td>18.42</td>\n",
       "      <td>82.61</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.03766</td>\n",
       "      <td>0.025620</td>\n",
       "      <td>0.029230</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>0.05863</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>2.3420</td>\n",
       "      <td>1.170</td>\n",
       "      <td>14.160</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.013430</td>\n",
       "      <td>0.011640</td>\n",
       "      <td>0.02671</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>13.300</td>\n",
       "      <td>22.81</td>\n",
       "      <td>84.46</td>\n",
       "      <td>545.9</td>\n",
       "      <td>0.09701</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.048330</td>\n",
       "      <td>0.05013</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>13.480</td>\n",
       "      <td>20.82</td>\n",
       "      <td>88.40</td>\n",
       "      <td>559.2</td>\n",
       "      <td>0.10160</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.054390</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.06419</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.5914</td>\n",
       "      <td>1.545</td>\n",
       "      <td>18.520</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.030490</td>\n",
       "      <td>0.012620</td>\n",
       "      <td>0.01377</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>15.530</td>\n",
       "      <td>26.02</td>\n",
       "      <td>107.30</td>\n",
       "      <td>740.4</td>\n",
       "      <td>0.16100</td>\n",
       "      <td>0.42250</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.22580</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.10710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>13.440</td>\n",
       "      <td>21.58</td>\n",
       "      <td>86.18</td>\n",
       "      <td>563.0</td>\n",
       "      <td>0.08162</td>\n",
       "      <td>0.06031</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.1784</td>\n",
       "      <td>0.05587</td>\n",
       "      <td>0.2385</td>\n",
       "      <td>0.8265</td>\n",
       "      <td>1.572</td>\n",
       "      <td>20.530</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.011020</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>0.01380</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>15.930</td>\n",
       "      <td>30.25</td>\n",
       "      <td>102.50</td>\n",
       "      <td>787.9</td>\n",
       "      <td>0.10940</td>\n",
       "      <td>0.20430</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.11120</td>\n",
       "      <td>0.2994</td>\n",
       "      <td>0.07146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10.950</td>\n",
       "      <td>21.35</td>\n",
       "      <td>71.90</td>\n",
       "      <td>371.1</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.056690</td>\n",
       "      <td>0.1895</td>\n",
       "      <td>0.06870</td>\n",
       "      <td>0.2366</td>\n",
       "      <td>1.4280</td>\n",
       "      <td>1.822</td>\n",
       "      <td>16.970</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.017640</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>0.010370</td>\n",
       "      <td>0.01357</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>12.840</td>\n",
       "      <td>35.34</td>\n",
       "      <td>87.22</td>\n",
       "      <td>514.0</td>\n",
       "      <td>0.19090</td>\n",
       "      <td>0.26980</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>0.14240</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>0.09606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>13.280</td>\n",
       "      <td>20.28</td>\n",
       "      <td>87.32</td>\n",
       "      <td>545.2</td>\n",
       "      <td>0.10410</td>\n",
       "      <td>0.14360</td>\n",
       "      <td>0.098470</td>\n",
       "      <td>0.061580</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.8249</td>\n",
       "      <td>2.427</td>\n",
       "      <td>31.330</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.009560</td>\n",
       "      <td>0.01719</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>17.380</td>\n",
       "      <td>28.00</td>\n",
       "      <td>113.10</td>\n",
       "      <td>907.2</td>\n",
       "      <td>0.15300</td>\n",
       "      <td>0.37240</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.14920</td>\n",
       "      <td>0.3739</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13.170</td>\n",
       "      <td>21.81</td>\n",
       "      <td>85.42</td>\n",
       "      <td>531.5</td>\n",
       "      <td>0.09714</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.082590</td>\n",
       "      <td>0.052520</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.06177</td>\n",
       "      <td>0.1938</td>\n",
       "      <td>0.6123</td>\n",
       "      <td>1.334</td>\n",
       "      <td>14.490</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.013840</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.01113</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>16.230</td>\n",
       "      <td>29.89</td>\n",
       "      <td>105.50</td>\n",
       "      <td>740.7</td>\n",
       "      <td>0.15030</td>\n",
       "      <td>0.39040</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.16070</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.09618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>18.650</td>\n",
       "      <td>17.60</td>\n",
       "      <td>123.70</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.16860</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>0.06049</td>\n",
       "      <td>0.6289</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>4.293</td>\n",
       "      <td>71.560</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.039940</td>\n",
       "      <td>0.055540</td>\n",
       "      <td>0.016950</td>\n",
       "      <td>0.02428</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>22.820</td>\n",
       "      <td>21.32</td>\n",
       "      <td>150.60</td>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.16790</td>\n",
       "      <td>0.50900</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.23780</td>\n",
       "      <td>0.3799</td>\n",
       "      <td>0.09185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8.196</td>\n",
       "      <td>16.84</td>\n",
       "      <td>51.71</td>\n",
       "      <td>201.9</td>\n",
       "      <td>0.08600</td>\n",
       "      <td>0.05943</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.06503</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.9567</td>\n",
       "      <td>1.094</td>\n",
       "      <td>8.205</td>\n",
       "      <td>0.008968</td>\n",
       "      <td>0.016460</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.02574</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>8.964</td>\n",
       "      <td>21.96</td>\n",
       "      <td>57.26</td>\n",
       "      <td>242.2</td>\n",
       "      <td>0.12970</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.07409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>13.170</td>\n",
       "      <td>18.66</td>\n",
       "      <td>85.98</td>\n",
       "      <td>534.6</td>\n",
       "      <td>0.11580</td>\n",
       "      <td>0.12310</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.06777</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>1.897</td>\n",
       "      <td>24.250</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>0.023360</td>\n",
       "      <td>0.029050</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>0.01743</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>15.670</td>\n",
       "      <td>27.95</td>\n",
       "      <td>102.80</td>\n",
       "      <td>759.4</td>\n",
       "      <td>0.17860</td>\n",
       "      <td>0.41660</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.20880</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.11790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>12.050</td>\n",
       "      <td>14.63</td>\n",
       "      <td>78.04</td>\n",
       "      <td>449.3</td>\n",
       "      <td>0.10310</td>\n",
       "      <td>0.09092</td>\n",
       "      <td>0.065920</td>\n",
       "      <td>0.027490</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.06043</td>\n",
       "      <td>0.2636</td>\n",
       "      <td>0.7294</td>\n",
       "      <td>1.848</td>\n",
       "      <td>19.870</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>0.023220</td>\n",
       "      <td>0.005660</td>\n",
       "      <td>0.01428</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>13.760</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.88</td>\n",
       "      <td>582.6</td>\n",
       "      <td>0.14940</td>\n",
       "      <td>0.21560</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.06548</td>\n",
       "      <td>0.2747</td>\n",
       "      <td>0.08301</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>13.490</td>\n",
       "      <td>22.30</td>\n",
       "      <td>86.91</td>\n",
       "      <td>561.0</td>\n",
       "      <td>0.08752</td>\n",
       "      <td>0.07698</td>\n",
       "      <td>0.047510</td>\n",
       "      <td>0.033840</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05718</td>\n",
       "      <td>0.2338</td>\n",
       "      <td>1.3530</td>\n",
       "      <td>1.735</td>\n",
       "      <td>20.200</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>0.020950</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>0.01641</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>15.150</td>\n",
       "      <td>31.82</td>\n",
       "      <td>99.00</td>\n",
       "      <td>698.8</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.17110</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.12820</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.06917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11.760</td>\n",
       "      <td>21.60</td>\n",
       "      <td>74.72</td>\n",
       "      <td>427.9</td>\n",
       "      <td>0.08637</td>\n",
       "      <td>0.04966</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.011150</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.05888</td>\n",
       "      <td>0.4062</td>\n",
       "      <td>1.2100</td>\n",
       "      <td>2.635</td>\n",
       "      <td>28.470</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>0.007445</td>\n",
       "      <td>0.02406</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>12.980</td>\n",
       "      <td>25.72</td>\n",
       "      <td>82.98</td>\n",
       "      <td>516.5</td>\n",
       "      <td>0.10850</td>\n",
       "      <td>0.08615</td>\n",
       "      <td>0.055230</td>\n",
       "      <td>0.03715</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>0.06563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>13.640</td>\n",
       "      <td>16.34</td>\n",
       "      <td>87.21</td>\n",
       "      <td>571.8</td>\n",
       "      <td>0.07685</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>0.018570</td>\n",
       "      <td>0.017230</td>\n",
       "      <td>0.1353</td>\n",
       "      <td>0.05953</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>1.449</td>\n",
       "      <td>14.550</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.01325</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>14.670</td>\n",
       "      <td>23.19</td>\n",
       "      <td>96.08</td>\n",
       "      <td>656.7</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.15820</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.08586</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.08025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>11.940</td>\n",
       "      <td>18.24</td>\n",
       "      <td>75.71</td>\n",
       "      <td>437.6</td>\n",
       "      <td>0.08261</td>\n",
       "      <td>0.04751</td>\n",
       "      <td>0.019720</td>\n",
       "      <td>0.013490</td>\n",
       "      <td>0.1868</td>\n",
       "      <td>0.06110</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>1.520</td>\n",
       "      <td>17.470</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.013110</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.01996</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>13.100</td>\n",
       "      <td>21.33</td>\n",
       "      <td>83.67</td>\n",
       "      <td>527.2</td>\n",
       "      <td>0.11440</td>\n",
       "      <td>0.08906</td>\n",
       "      <td>0.092030</td>\n",
       "      <td>0.06296</td>\n",
       "      <td>0.2785</td>\n",
       "      <td>0.07408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15.100</td>\n",
       "      <td>22.02</td>\n",
       "      <td>97.26</td>\n",
       "      <td>712.8</td>\n",
       "      <td>0.09056</td>\n",
       "      <td>0.07081</td>\n",
       "      <td>0.052530</td>\n",
       "      <td>0.033340</td>\n",
       "      <td>0.1616</td>\n",
       "      <td>0.05684</td>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.8339</td>\n",
       "      <td>2.097</td>\n",
       "      <td>29.910</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.016030</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>0.01095</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>18.100</td>\n",
       "      <td>31.69</td>\n",
       "      <td>117.70</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.20570</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.15300</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>0.07873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>11.520</td>\n",
       "      <td>18.75</td>\n",
       "      <td>73.34</td>\n",
       "      <td>409.0</td>\n",
       "      <td>0.09524</td>\n",
       "      <td>0.05473</td>\n",
       "      <td>0.030360</td>\n",
       "      <td>0.022780</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>0.05907</td>\n",
       "      <td>0.3249</td>\n",
       "      <td>0.9591</td>\n",
       "      <td>2.183</td>\n",
       "      <td>23.470</td>\n",
       "      <td>0.008328</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.013490</td>\n",
       "      <td>0.008670</td>\n",
       "      <td>0.03218</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>12.840</td>\n",
       "      <td>22.47</td>\n",
       "      <td>81.81</td>\n",
       "      <td>506.2</td>\n",
       "      <td>0.12490</td>\n",
       "      <td>0.08720</td>\n",
       "      <td>0.090760</td>\n",
       "      <td>0.06316</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>0.07036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>14.710</td>\n",
       "      <td>21.59</td>\n",
       "      <td>95.55</td>\n",
       "      <td>656.9</td>\n",
       "      <td>0.11370</td>\n",
       "      <td>0.13650</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.081230</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>0.06758</td>\n",
       "      <td>0.4226</td>\n",
       "      <td>1.1500</td>\n",
       "      <td>2.735</td>\n",
       "      <td>40.090</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.028550</td>\n",
       "      <td>0.025720</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>17.870</td>\n",
       "      <td>30.70</td>\n",
       "      <td>115.70</td>\n",
       "      <td>985.5</td>\n",
       "      <td>0.13680</td>\n",
       "      <td>0.42900</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.18340</td>\n",
       "      <td>0.3698</td>\n",
       "      <td>0.10940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>13.050</td>\n",
       "      <td>19.31</td>\n",
       "      <td>82.61</td>\n",
       "      <td>527.2</td>\n",
       "      <td>0.08060</td>\n",
       "      <td>0.03789</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.1819</td>\n",
       "      <td>0.05501</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>1.2140</td>\n",
       "      <td>2.595</td>\n",
       "      <td>32.960</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.02190</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>14.230</td>\n",
       "      <td>22.25</td>\n",
       "      <td>90.24</td>\n",
       "      <td>624.1</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.01111</td>\n",
       "      <td>0.2439</td>\n",
       "      <td>0.06289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>8.618</td>\n",
       "      <td>11.79</td>\n",
       "      <td>54.34</td>\n",
       "      <td>224.5</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.05272</td>\n",
       "      <td>0.020610</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>0.07187</td>\n",
       "      <td>0.1559</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>1.046</td>\n",
       "      <td>8.322</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.010550</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>0.005742</td>\n",
       "      <td>0.02090</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>9.507</td>\n",
       "      <td>15.40</td>\n",
       "      <td>59.90</td>\n",
       "      <td>274.9</td>\n",
       "      <td>0.17330</td>\n",
       "      <td>0.12390</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.04419</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.09026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10.170</td>\n",
       "      <td>14.88</td>\n",
       "      <td>64.55</td>\n",
       "      <td>311.9</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.08061</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.06960</td>\n",
       "      <td>0.5158</td>\n",
       "      <td>1.4410</td>\n",
       "      <td>3.312</td>\n",
       "      <td>34.620</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.010990</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>0.04183</td>\n",
       "      <td>0.005953</td>\n",
       "      <td>11.020</td>\n",
       "      <td>17.45</td>\n",
       "      <td>69.86</td>\n",
       "      <td>368.6</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.02579</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>0.08020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>8.598</td>\n",
       "      <td>20.98</td>\n",
       "      <td>54.66</td>\n",
       "      <td>221.8</td>\n",
       "      <td>0.12430</td>\n",
       "      <td>0.08963</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.1828</td>\n",
       "      <td>0.06757</td>\n",
       "      <td>0.3582</td>\n",
       "      <td>2.0670</td>\n",
       "      <td>2.493</td>\n",
       "      <td>18.390</td>\n",
       "      <td>0.011930</td>\n",
       "      <td>0.031620</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.03357</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>9.565</td>\n",
       "      <td>27.04</td>\n",
       "      <td>62.06</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.16390</td>\n",
       "      <td>0.16980</td>\n",
       "      <td>0.090010</td>\n",
       "      <td>0.02778</td>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.07712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>14.250</td>\n",
       "      <td>22.15</td>\n",
       "      <td>96.42</td>\n",
       "      <td>645.7</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.20080</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>0.086530</td>\n",
       "      <td>0.1949</td>\n",
       "      <td>0.07292</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>1.2680</td>\n",
       "      <td>5.373</td>\n",
       "      <td>60.780</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.070560</td>\n",
       "      <td>0.068990</td>\n",
       "      <td>0.018480</td>\n",
       "      <td>0.01700</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>17.670</td>\n",
       "      <td>29.51</td>\n",
       "      <td>119.10</td>\n",
       "      <td>959.5</td>\n",
       "      <td>0.16400</td>\n",
       "      <td>0.62470</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.17850</td>\n",
       "      <td>0.2844</td>\n",
       "      <td>0.11320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>9.173</td>\n",
       "      <td>13.86</td>\n",
       "      <td>59.20</td>\n",
       "      <td>260.9</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.06963</td>\n",
       "      <td>0.4098</td>\n",
       "      <td>2.2650</td>\n",
       "      <td>2.608</td>\n",
       "      <td>23.520</td>\n",
       "      <td>0.008738</td>\n",
       "      <td>0.039380</td>\n",
       "      <td>0.043120</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.04192</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>10.010</td>\n",
       "      <td>19.23</td>\n",
       "      <td>65.59</td>\n",
       "      <td>310.1</td>\n",
       "      <td>0.09836</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.05087</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.08490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "1        20.570         17.77          132.90     1326.0          0.08474   \n",
       "3        11.420         20.38           77.58      386.1          0.14250   \n",
       "5        12.450         15.70           82.57      477.1          0.12780   \n",
       "6        18.250         19.98          119.60     1040.0          0.09463   \n",
       "7        13.710         20.83           90.20      577.9          0.11890   \n",
       "8        13.000         21.82           87.50      519.8          0.12730   \n",
       "9        12.460         24.04           83.97      475.9          0.11860   \n",
       "10       16.020         23.24          102.70      797.8          0.08206   \n",
       "11       15.780         17.89          103.60      781.0          0.09710   \n",
       "13       15.850         23.95          103.70      782.7          0.08401   \n",
       "14       13.730         22.61           93.60      578.3          0.11310   \n",
       "15       14.540         27.54           96.73      658.8          0.11390   \n",
       "16       14.680         20.13           94.74      684.5          0.09867   \n",
       "17       16.130         20.68          108.10      798.8          0.11700   \n",
       "19       13.540         14.36           87.46      566.3          0.09779   \n",
       "20       13.080         15.71           85.63      520.0          0.10750   \n",
       "21        9.504         12.44           60.34      273.9          0.10240   \n",
       "22       15.340         14.26          102.50      704.4          0.10730   \n",
       "26       14.580         21.53           97.41      644.8          0.10540   \n",
       "28       15.300         25.27          102.40      732.4          0.10820   \n",
       "29       17.570         15.05          115.00      955.1          0.09847   \n",
       "31       11.840         18.70           77.93      440.6          0.11090   \n",
       "32       17.020         23.98          112.80      899.3          0.11970   \n",
       "33       19.270         26.47          127.90     1162.0          0.09401   \n",
       "34       16.130         17.88          107.00      807.2          0.10400   \n",
       "35       16.740         21.59          110.10      869.5          0.09610   \n",
       "36       14.250         21.72           93.63      633.0          0.09823   \n",
       "37       13.030         18.42           82.61      523.8          0.08983   \n",
       "39       13.480         20.82           88.40      559.2          0.10160   \n",
       "40       13.440         21.58           86.18      563.0          0.08162   \n",
       "41       10.950         21.35           71.90      371.1          0.12270   \n",
       "43       13.280         20.28           87.32      545.2          0.10410   \n",
       "44       13.170         21.81           85.42      531.5          0.09714   \n",
       "45       18.650         17.60          123.70     1076.0          0.10990   \n",
       "46        8.196         16.84           51.71      201.9          0.08600   \n",
       "47       13.170         18.66           85.98      534.6          0.11580   \n",
       "48       12.050         14.63           78.04      449.3          0.10310   \n",
       "49       13.490         22.30           86.91      561.0          0.08752   \n",
       "50       11.760         21.60           74.72      427.9          0.08637   \n",
       "51       13.640         16.34           87.21      571.8          0.07685   \n",
       "52       11.940         18.24           75.71      437.6          0.08261   \n",
       "54       15.100         22.02           97.26      712.8          0.09056   \n",
       "55       11.520         18.75           73.34      409.0          0.09524   \n",
       "57       14.710         21.59           95.55      656.9          0.11370   \n",
       "58       13.050         19.31           82.61      527.2          0.08060   \n",
       "59        8.618         11.79           54.34      224.5          0.09752   \n",
       "60       10.170         14.88           64.55      311.9          0.11340   \n",
       "61        8.598         20.98           54.66      221.8          0.12430   \n",
       "62       14.250         22.15           96.42      645.7          0.10490   \n",
       "63        9.173         13.86           59.20      260.9          0.07721   \n",
       "\n",
       "    compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "1            0.07864        0.086900             0.070170         0.1812   \n",
       "3            0.28390        0.241400             0.105200         0.2597   \n",
       "5            0.17000        0.157800             0.080890         0.2087   \n",
       "6            0.10900        0.112700             0.074000         0.1794   \n",
       "7            0.16450        0.093660             0.059850         0.2196   \n",
       "8            0.19320        0.185900             0.093530         0.2350   \n",
       "9            0.23960        0.227300             0.085430         0.2030   \n",
       "10           0.06669        0.032990             0.033230         0.1528   \n",
       "11           0.12920        0.099540             0.066060         0.1842   \n",
       "13           0.10020        0.099380             0.053640         0.1847   \n",
       "14           0.22930        0.212800             0.080250         0.2069   \n",
       "15           0.15950        0.163900             0.073640         0.2303   \n",
       "16           0.07200        0.073950             0.052590         0.1586   \n",
       "17           0.20220        0.172200             0.102800         0.2164   \n",
       "19           0.08129        0.066640             0.047810         0.1885   \n",
       "20           0.12700        0.045680             0.031100         0.1967   \n",
       "21           0.06492        0.029560             0.020760         0.1815   \n",
       "22           0.21350        0.207700             0.097560         0.2521   \n",
       "26           0.18680        0.142500             0.087830         0.2252   \n",
       "28           0.16970        0.168300             0.087510         0.1926   \n",
       "29           0.11570        0.098750             0.079530         0.1739   \n",
       "31           0.15160        0.121800             0.051820         0.2301   \n",
       "32           0.14960        0.241700             0.120300         0.2248   \n",
       "33           0.17190        0.165700             0.075930         0.1853   \n",
       "34           0.15590        0.135400             0.077520         0.1998   \n",
       "35           0.13360        0.134800             0.060180         0.1896   \n",
       "36           0.10980        0.131900             0.055980         0.1885   \n",
       "37           0.03766        0.025620             0.029230         0.1467   \n",
       "39           0.12550        0.106300             0.054390         0.1720   \n",
       "40           0.06031        0.031100             0.020310         0.1784   \n",
       "41           0.12180        0.104400             0.056690         0.1895   \n",
       "43           0.14360        0.098470             0.061580         0.1974   \n",
       "44           0.10470        0.082590             0.052520         0.1746   \n",
       "45           0.16860        0.197400             0.100900         0.1907   \n",
       "46           0.05943        0.015880             0.005917         0.1769   \n",
       "47           0.12310        0.122600             0.073400         0.2128   \n",
       "48           0.09092        0.065920             0.027490         0.1675   \n",
       "49           0.07698        0.047510             0.033840         0.1809   \n",
       "50           0.04966        0.016570             0.011150         0.1495   \n",
       "51           0.06059        0.018570             0.017230         0.1353   \n",
       "52           0.04751        0.019720             0.013490         0.1868   \n",
       "54           0.07081        0.052530             0.033340         0.1616   \n",
       "55           0.05473        0.030360             0.022780         0.1920   \n",
       "57           0.13650        0.129300             0.081230         0.2027   \n",
       "58           0.03789        0.000692             0.004167         0.1819   \n",
       "59           0.05272        0.020610             0.007799         0.1683   \n",
       "60           0.08061        0.010840             0.012900         0.2743   \n",
       "61           0.08963        0.030000             0.009259         0.1828   \n",
       "62           0.20080        0.213500             0.086530         0.1949   \n",
       "63           0.08751        0.059880             0.021800         0.2341   \n",
       "\n",
       "    fractal_dimension_mean  radius_se  texture_se  perimeter_se  area_se  \\\n",
       "1                  0.05667     0.5435      0.7339         3.398   74.080   \n",
       "3                  0.09744     0.4956      1.1560         3.445   27.230   \n",
       "5                  0.07613     0.3345      0.8902         2.217   27.190   \n",
       "6                  0.05742     0.4467      0.7732         3.180   53.910   \n",
       "7                  0.07451     0.5835      1.3770         3.856   50.960   \n",
       "8                  0.07389     0.3063      1.0020         2.406   24.320   \n",
       "9                  0.08243     0.2976      1.5990         2.039   23.940   \n",
       "10                 0.05697     0.3795      1.1870         2.466   40.510   \n",
       "11                 0.06082     0.5058      0.9849         3.564   54.160   \n",
       "13                 0.05338     0.4033      1.0780         2.903   36.580   \n",
       "14                 0.07682     0.2121      1.1690         2.061   19.210   \n",
       "15                 0.07077     0.3700      1.0330         2.879   32.550   \n",
       "16                 0.05922     0.4727      1.2400         3.195   45.400   \n",
       "17                 0.07356     0.5692      1.0730         3.854   54.180   \n",
       "19                 0.05766     0.2699      0.7886         2.058   23.560   \n",
       "20                 0.06811     0.1852      0.7477         1.383   14.670   \n",
       "21                 0.06905     0.2773      0.9768         1.909   15.700   \n",
       "22                 0.07032     0.4388      0.7096         3.384   44.910   \n",
       "26                 0.06924     0.2545      0.9832         2.110   21.050   \n",
       "28                 0.06540     0.4390      1.0120         3.498   43.500   \n",
       "29                 0.06149     0.6003      0.8225         4.655   61.100   \n",
       "31                 0.07799     0.4825      1.0300         3.475   41.000   \n",
       "32                 0.06382     0.6009      1.3980         3.999   67.780   \n",
       "33                 0.06261     0.5558      0.6062         3.528   68.170   \n",
       "34                 0.06515     0.3340      0.6857         2.183   35.030   \n",
       "35                 0.05656     0.4615      0.9197         3.008   45.190   \n",
       "36                 0.06125     0.2860      1.0190         2.657   24.910   \n",
       "37                 0.05863     0.1839      2.3420         1.170   14.160   \n",
       "39                 0.06419     0.2130      0.5914         1.545   18.520   \n",
       "40                 0.05587     0.2385      0.8265         1.572   20.530   \n",
       "41                 0.06870     0.2366      1.4280         1.822   16.970   \n",
       "43                 0.06782     0.3704      0.8249         2.427   31.330   \n",
       "44                 0.06177     0.1938      0.6123         1.334   14.490   \n",
       "45                 0.06049     0.6289      0.6633         4.293   71.560   \n",
       "46                 0.06503     0.1563      0.9567         1.094    8.205   \n",
       "47                 0.06777     0.2871      0.8937         1.897   24.250   \n",
       "48                 0.06043     0.2636      0.7294         1.848   19.870   \n",
       "49                 0.05718     0.2338      1.3530         1.735   20.200   \n",
       "50                 0.05888     0.4062      1.2100         2.635   28.470   \n",
       "51                 0.05953     0.1872      0.9234         1.449   14.550   \n",
       "52                 0.06110     0.2273      0.6329         1.520   17.470   \n",
       "54                 0.05684     0.3105      0.8339         2.097   29.910   \n",
       "55                 0.05907     0.3249      0.9591         2.183   23.470   \n",
       "57                 0.06758     0.4226      1.1500         2.735   40.090   \n",
       "58                 0.05501     0.4040      1.2140         2.595   32.960   \n",
       "59                 0.07187     0.1559      0.5796         1.046    8.322   \n",
       "60                 0.06960     0.5158      1.4410         3.312   34.620   \n",
       "61                 0.06757     0.3582      2.0670         2.493   18.390   \n",
       "62                 0.07292     0.7036      1.2680         5.373   60.780   \n",
       "63                 0.06963     0.4098      2.2650         2.608   23.520   \n",
       "\n",
       "    smoothness_se  compactness_se  concavity_se  concave points_se  \\\n",
       "1        0.005225        0.013080      0.018600           0.013400   \n",
       "3        0.009110        0.074580      0.056610           0.018670   \n",
       "5        0.007510        0.033450      0.036720           0.011370   \n",
       "6        0.004314        0.013820      0.022540           0.010390   \n",
       "7        0.008805        0.030290      0.024880           0.014480   \n",
       "8        0.005731        0.035020      0.035530           0.012260   \n",
       "9        0.007149        0.072170      0.077430           0.014320   \n",
       "10       0.004029        0.009269      0.011010           0.007591   \n",
       "11       0.005771        0.040610      0.027910           0.012820   \n",
       "13       0.009769        0.031260      0.050510           0.019920   \n",
       "14       0.006429        0.059360      0.055010           0.016280   \n",
       "15       0.005607        0.042400      0.047410           0.010900   \n",
       "16       0.005718        0.011620      0.019980           0.011090   \n",
       "17       0.007026        0.025010      0.031880           0.012970   \n",
       "19       0.008462        0.014600      0.023870           0.013150   \n",
       "20       0.004097        0.018980      0.016980           0.006490   \n",
       "21       0.009606        0.014320      0.019850           0.014210   \n",
       "22       0.006789        0.053280      0.064460           0.022520   \n",
       "26       0.004452        0.030550      0.026810           0.013520   \n",
       "28       0.005233        0.030570      0.035760           0.010830   \n",
       "29       0.005627        0.030330      0.034070           0.013540   \n",
       "31       0.005551        0.034140      0.042050           0.010440   \n",
       "32       0.008268        0.030820      0.050420           0.011120   \n",
       "33       0.005015        0.033180      0.034970           0.009643   \n",
       "34       0.004185        0.028680      0.026640           0.009067   \n",
       "35       0.005776        0.024990      0.036950           0.011950   \n",
       "36       0.005878        0.029950      0.048150           0.011610   \n",
       "37       0.004352        0.004899      0.013430           0.011640   \n",
       "39       0.005367        0.022390      0.030490           0.012620   \n",
       "40       0.003280        0.011020      0.013900           0.006881   \n",
       "41       0.008064        0.017640      0.025950           0.010370   \n",
       "43       0.005072        0.021470      0.021850           0.009560   \n",
       "44       0.003350        0.013840      0.014520           0.006853   \n",
       "45       0.006294        0.039940      0.055540           0.016950   \n",
       "46       0.008968        0.016460      0.015880           0.005917   \n",
       "47       0.006532        0.023360      0.029050           0.012150   \n",
       "48       0.005488        0.014270      0.023220           0.005660   \n",
       "49       0.004455        0.013820      0.020950           0.011840   \n",
       "50       0.005857        0.009758      0.011680           0.007445   \n",
       "51       0.004477        0.011770      0.010790           0.007956   \n",
       "52       0.007210        0.008380      0.013110           0.008000   \n",
       "54       0.004675        0.010300      0.016030           0.009222   \n",
       "55       0.008328        0.008722      0.013490           0.008670   \n",
       "57       0.003659        0.028550      0.025720           0.012720   \n",
       "58       0.007491        0.008593      0.000692           0.004167   \n",
       "59       0.010110        0.010550      0.019810           0.005742   \n",
       "60       0.007514        0.010990      0.007665           0.008193   \n",
       "61       0.011930        0.031620      0.030000           0.009259   \n",
       "62       0.009407        0.070560      0.068990           0.018480   \n",
       "63       0.008738        0.039380      0.043120           0.015600   \n",
       "\n",
       "    symmetry_se  fractal_dimension_se  radius_worst  texture_worst  \\\n",
       "1       0.01389              0.003532        24.990          23.41   \n",
       "3       0.05963              0.009208        14.910          26.50   \n",
       "5       0.02165              0.005082        15.470          23.75   \n",
       "6       0.01369              0.002179        22.880          27.66   \n",
       "7       0.01486              0.005412        17.060          28.14   \n",
       "8       0.02143              0.003749        15.490          30.73   \n",
       "9       0.01789              0.010080        15.090          40.68   \n",
       "10      0.01460              0.003042        19.190          33.88   \n",
       "11      0.02008              0.004144        20.420          27.28   \n",
       "13      0.02981              0.003002        16.840          27.66   \n",
       "14      0.01961              0.008093        15.030          32.01   \n",
       "15      0.01857              0.005466        17.460          37.13   \n",
       "16      0.01410              0.002085        19.070          30.88   \n",
       "17      0.01689              0.004142        20.960          31.48   \n",
       "19      0.01980              0.002300        15.110          19.26   \n",
       "20      0.01678              0.002425        14.500          20.49   \n",
       "21      0.02027              0.002968        10.230          15.66   \n",
       "22      0.03672              0.004394        18.070          19.08   \n",
       "26      0.01454              0.003711        17.620          33.21   \n",
       "28      0.01768              0.002967        20.270          36.71   \n",
       "29      0.01925              0.003742        20.010          19.52   \n",
       "31      0.02273              0.005667        16.820          28.12   \n",
       "32      0.02102              0.003854        20.880          32.09   \n",
       "33      0.01543              0.003896        24.150          30.90   \n",
       "34      0.01703              0.003817        20.210          27.26   \n",
       "35      0.02789              0.002665        20.010          29.02   \n",
       "36      0.02028              0.004022        15.890          30.36   \n",
       "37      0.02671              0.001777        13.300          22.81   \n",
       "39      0.01377              0.003187        15.530          26.02   \n",
       "40      0.01380              0.001286        15.930          30.25   \n",
       "41      0.01357              0.003040        12.840          35.34   \n",
       "43      0.01719              0.003317        17.380          28.00   \n",
       "44      0.01113              0.001720        16.230          29.89   \n",
       "45      0.02428              0.003535        22.820          21.32   \n",
       "46      0.02574              0.002582         8.964          21.96   \n",
       "47      0.01743              0.003643        15.670          27.95   \n",
       "48      0.01428              0.002422        13.760          20.70   \n",
       "49      0.01641              0.001956        15.150          31.82   \n",
       "50      0.02406              0.001769        12.980          25.72   \n",
       "51      0.01325              0.002551        14.670          23.19   \n",
       "52      0.01996              0.002635        13.100          21.33   \n",
       "54      0.01095              0.001629        18.100          31.69   \n",
       "55      0.03218              0.002386        12.840          22.47   \n",
       "57      0.01817              0.004108        17.870          30.70   \n",
       "58      0.02190              0.002990        14.230          22.25   \n",
       "59      0.02090              0.002788         9.507          15.40   \n",
       "60      0.04183              0.005953        11.020          17.45   \n",
       "61      0.03357              0.003048         9.565          27.04   \n",
       "62      0.01700              0.006113        17.670          29.51   \n",
       "63      0.04192              0.005822        10.010          19.23   \n",
       "\n",
       "    perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "1            158.80      1956.0           0.12380            0.18660   \n",
       "3             98.87       567.7           0.20980            0.86630   \n",
       "5            103.40       741.6           0.17910            0.52490   \n",
       "6            153.20      1606.0           0.14420            0.25760   \n",
       "7            110.60       897.0           0.16540            0.36820   \n",
       "8            106.20       739.3           0.17030            0.54010   \n",
       "9             97.65       711.4           0.18530            1.05800   \n",
       "10           123.80      1150.0           0.11810            0.15510   \n",
       "11           136.50      1299.0           0.13960            0.56090   \n",
       "13           112.00       876.5           0.11310            0.19240   \n",
       "14           108.80       697.7           0.16510            0.77250   \n",
       "15           124.10       943.2           0.16780            0.65770   \n",
       "16           123.40      1138.0           0.14640            0.18710   \n",
       "17           136.80      1315.0           0.17890            0.42330   \n",
       "19            99.70       711.2           0.14400            0.17730   \n",
       "20            96.09       630.5           0.13120            0.27760   \n",
       "21            65.13       314.9           0.13240            0.11480   \n",
       "22           125.10       980.9           0.13900            0.59540   \n",
       "26           122.40       896.9           0.15250            0.66430   \n",
       "28           149.30      1269.0           0.16410            0.61100   \n",
       "29           134.90      1227.0           0.12550            0.28120   \n",
       "31           119.40       888.7           0.16370            0.57750   \n",
       "32           136.10      1344.0           0.16340            0.35590   \n",
       "33           161.40      1813.0           0.15090            0.65900   \n",
       "34           132.70      1261.0           0.14460            0.58040   \n",
       "35           133.50      1229.0           0.15630            0.38350   \n",
       "36           116.20       799.6           0.14460            0.42380   \n",
       "37            84.46       545.9           0.09701            0.04619   \n",
       "39           107.30       740.4           0.16100            0.42250   \n",
       "40           102.50       787.9           0.10940            0.20430   \n",
       "41            87.22       514.0           0.19090            0.26980   \n",
       "43           113.10       907.2           0.15300            0.37240   \n",
       "44           105.50       740.7           0.15030            0.39040   \n",
       "45           150.60      1567.0           0.16790            0.50900   \n",
       "46            57.26       242.2           0.12970            0.13570   \n",
       "47           102.80       759.4           0.17860            0.41660   \n",
       "48            89.88       582.6           0.14940            0.21560   \n",
       "49            99.00       698.8           0.11620            0.17110   \n",
       "50            82.98       516.5           0.10850            0.08615   \n",
       "51            96.08       656.7           0.10890            0.15820   \n",
       "52            83.67       527.2           0.11440            0.08906   \n",
       "54           117.70      1030.0           0.13890            0.20570   \n",
       "55            81.81       506.2           0.12490            0.08720   \n",
       "57           115.70       985.5           0.13680            0.42900   \n",
       "58            90.24       624.1           0.10210            0.06191   \n",
       "59            59.90       274.9           0.17330            0.12390   \n",
       "60            69.86       368.6           0.12750            0.09866   \n",
       "61            62.06       273.9           0.16390            0.16980   \n",
       "62           119.10       959.5           0.16400            0.62470   \n",
       "63            65.59       310.1           0.09836            0.16780   \n",
       "\n",
       "    concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "1          0.241600               0.18600          0.2750   \n",
       "3          0.686900               0.25750          0.6638   \n",
       "5          0.535500               0.17410          0.3985   \n",
       "6          0.378400               0.19320          0.3063   \n",
       "7          0.267800               0.15560          0.3196   \n",
       "8          0.539000               0.20600          0.4378   \n",
       "9          1.105000               0.22100          0.4366   \n",
       "10         0.145900               0.09975          0.2948   \n",
       "11         0.396500               0.18100          0.3792   \n",
       "13         0.232200               0.11190          0.2809   \n",
       "14         0.694300               0.22080          0.3596   \n",
       "15         0.702600               0.17120          0.4218   \n",
       "16         0.291400               0.16090          0.3029   \n",
       "17         0.478400               0.20730          0.3706   \n",
       "19         0.239000               0.12880          0.2977   \n",
       "20         0.189000               0.07283          0.3184   \n",
       "21         0.088670               0.06227          0.2450   \n",
       "22         0.630500               0.23930          0.4667   \n",
       "26         0.553900               0.27010          0.4264   \n",
       "28         0.633500               0.20240          0.4027   \n",
       "29         0.248900               0.14560          0.2756   \n",
       "31         0.695600               0.15460          0.4761   \n",
       "32         0.558800               0.18470          0.3530   \n",
       "33         0.609100               0.17850          0.3672   \n",
       "34         0.527400               0.18640          0.4270   \n",
       "35         0.540900               0.18130          0.4863   \n",
       "36         0.518600               0.14470          0.3591   \n",
       "37         0.048330               0.05013          0.1987   \n",
       "39         0.503000               0.22580          0.2807   \n",
       "40         0.208500               0.11120          0.2994   \n",
       "41         0.402300               0.14240          0.2964   \n",
       "43         0.366400               0.14920          0.3739   \n",
       "44         0.372800               0.16070          0.3693   \n",
       "45         0.734500               0.23780          0.3799   \n",
       "46         0.068800               0.02564          0.3105   \n",
       "47         0.500600               0.20880          0.3900   \n",
       "48         0.305000               0.06548          0.2747   \n",
       "49         0.228200               0.12820          0.2871   \n",
       "50         0.055230               0.03715          0.2433   \n",
       "51         0.105000               0.08586          0.2346   \n",
       "52         0.092030               0.06296          0.2785   \n",
       "54         0.271200               0.15300          0.2675   \n",
       "55         0.090760               0.06316          0.3306   \n",
       "57         0.358700               0.18340          0.3698   \n",
       "58         0.001845               0.01111          0.2439   \n",
       "59         0.116800               0.04419          0.3220   \n",
       "60         0.021680               0.02579          0.3557   \n",
       "61         0.090010               0.02778          0.2972   \n",
       "62         0.692200               0.17850          0.2844   \n",
       "63         0.139700               0.05087          0.3282   \n",
       "\n",
       "    fractal_dimension_worst  diagnosis_M  \n",
       "1                   0.08902            1  \n",
       "3                   0.17300            1  \n",
       "5                   0.12440            1  \n",
       "6                   0.08368            1  \n",
       "7                   0.11510            1  \n",
       "8                   0.10720            1  \n",
       "9                   0.20750            1  \n",
       "10                  0.08452            1  \n",
       "11                  0.10480            1  \n",
       "13                  0.06287            1  \n",
       "14                  0.14310            1  \n",
       "15                  0.13410            1  \n",
       "16                  0.08216            1  \n",
       "17                  0.11420            1  \n",
       "19                  0.07259            0  \n",
       "20                  0.08183            0  \n",
       "21                  0.07773            0  \n",
       "22                  0.09946            1  \n",
       "26                  0.12750            1  \n",
       "28                  0.09876            1  \n",
       "29                  0.07919            1  \n",
       "31                  0.14020            1  \n",
       "32                  0.08482            1  \n",
       "33                  0.11230            1  \n",
       "34                  0.12330            1  \n",
       "35                  0.08633            1  \n",
       "36                  0.10140            1  \n",
       "37                  0.06169            0  \n",
       "39                  0.10710            1  \n",
       "40                  0.07146            1  \n",
       "41                  0.09606            1  \n",
       "43                  0.10270            1  \n",
       "44                  0.09618            1  \n",
       "45                  0.09185            1  \n",
       "46                  0.07409            0  \n",
       "47                  0.11790            1  \n",
       "48                  0.08301            0  \n",
       "49                  0.06917            0  \n",
       "50                  0.06563            0  \n",
       "51                  0.08025            0  \n",
       "52                  0.07408            0  \n",
       "54                  0.07873            1  \n",
       "55                  0.07036            0  \n",
       "57                  0.10940            1  \n",
       "58                  0.06289            0  \n",
       "59                  0.09026            0  \n",
       "60                  0.08020            0  \n",
       "61                  0.07712            0  \n",
       "62                  0.11320            1  \n",
       "63                  0.08490            0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b1c79",
   "metadata": {},
   "source": [
    "# TESTE - FUNCIONA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b1106cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Henrique\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Função para fazer a predição com LogisticRegression\n",
    "def breast_cancer_predict():\n",
    "    # Coleta os dados dos campos\n",
    "    try:\n",
    "        radius_m = float(radius_mean.get())\n",
    "        texture_m = float(texture_mean.get())\n",
    "        perimeter_m = float(perimeter_mean.get())\n",
    "        area_m = float(area_mean.get())\n",
    "        smoothness_m = float(smoothness_mean.get())\n",
    "        compactness_m = float(compactness_mean.get())\n",
    "        concavity_m = float(concavity_mean.get())\n",
    "        concave_points_m = float(concave_points_mean.get())\n",
    "        symmetry_m = float(symmetry_mean.get())\n",
    "        fractal_dimension_m = float(fractal_dimension_mean.get())\n",
    "        radius_s = float(radius_se.get())\n",
    "        texture_s = float(texture_se.get())\n",
    "        perimeter_s = float(perimeter_se.get())\n",
    "        area_s = float(area_se.get())\n",
    "        smoothness_s = float(smoothness_se.get())\n",
    "        compactness_s = float(compactness_se.get())\n",
    "        concavity_s = float(concavity_se.get())\n",
    "        concave_points_s = float(concave_points_se.get())\n",
    "        symmetry_s = float(symmetry_se.get())\n",
    "        fractal_dimension_s = float(fractal_dimension_se.get())\n",
    "        radius_w = float(radius_worst.get())\n",
    "        texture_w = float(texture_worst.get())\n",
    "        perimeter_w = float(perimeter_worst.get())\n",
    "        area_w = float(area_worst.get())\n",
    "        smoothness_w = float(smoothness_worst.get())\n",
    "        compactness_w = float(compactness_worst.get())\n",
    "        concavity_w = float(concavity_worst.get())\n",
    "        concave_points_w = float(concave_points_worst.get())\n",
    "        symmetry_w = float(symmetry_worst.get())\n",
    "        fractal_dimension_w = float(fractal_dimension_worst.get())      \n",
    "\n",
    "        # Crie um array numpy com os dados para a predição\n",
    "        data = np.array([[radius_m, texture_m, perimeter_m, area_m, smoothness_m, compactness_m, concavity_m, \n",
    "                          concave_points_m, symmetry_m, fractal_dimension_m, radius_s, texture_s, perimeter_s, area_s,\n",
    "                          smoothness_s, compactness_s, concavity_s, concave_points_s, symmetry_s, fractal_dimension_s, radius_w, texture_w,\n",
    "                          perimeter_w, area_w, smoothness_w, compactness_w, concavity_w, concave_points_w, symmetry_w, fractal_dimension_w]])\n",
    "\n",
    "        # Crie um modelo de LogisticRegression fictício (substitua pelo seu modelo real)\n",
    "        model = LogisticRegression(C=1.75, random_state=0, solver='sag')\n",
    "\n",
    "        # Treine o modelo (substitua pelo treinamento do seu modelo real)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Crie uma instância do StandardScaler\n",
    "        #scaler = StandardScaler()\n",
    "\n",
    "        # Ajuste o scaler aos seus dados de treinamento\n",
    "        #scaler.fit(X_train)\n",
    "\n",
    "        # Aplique o scaler aos dados de entrada do usuário\n",
    "        #data_scaled = scaler.transform(data)\n",
    "\n",
    "        # Faça a predição\n",
    "        prediction = model.predict(scaler.transform(data))\n",
    "\n",
    "        # Traduza a predição em 'malignant' ou 'benign'\n",
    "        if prediction[0] == 1:\n",
    "            result_text = 'Malignant'\n",
    "            result_bg_color = 'red'\n",
    "        else:\n",
    "            result_text = 'Benign'\n",
    "            result_bg_color = 'green'\n",
    "\n",
    "        # Atualize o texto do label_button com o resultado da predição\n",
    "        font_style_predict = (\"Arial\", 14)\n",
    "        label_button.config(text=f'{result_text}', relief='solid', font=font_style_predict, fg='white')\n",
    "        label_button.configure(bg=result_bg_color)\n",
    "\n",
    "    except ValueError:\n",
    "        # Tratamento de erro para campos vazios ou não numéricos\n",
    "        label_button.config(text='Error: Fill in all fields with numerical values', relief='solid', bg='red')\n",
    "        \n",
    "        \n",
    "program = tk.Tk()\n",
    "\n",
    "program.title('Breast Cancer Diagnostic')\n",
    "\n",
    "# Main label\n",
    "font_style = (\"Arial\", 20)\n",
    "label_main = tk.Label(text='PYHTON CANCER INSTITUTE\\nBreast Cancer Diagnostic System', borderwidth=1, relief='solid', font=font_style, bg='#658bd2', fg='white')\n",
    "label_main.grid(row=0, column=0, padx=10, pady=10,sticky='nswe', columnspan=10)\n",
    "\n",
    "# Columns - first row\n",
    "label_radius_mean = tk.Label(text='radius_mean')\n",
    "label_radius_mean.grid(row=1, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "radius_mean = tk.Entry()\n",
    "radius_mean.grid(row=1, column=1)\n",
    "\n",
    "label_texture_mean = tk.Label(text='texture_mean')\n",
    "label_texture_mean.grid(row=1, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "texture_mean = tk.Entry()\n",
    "texture_mean.grid(row=1, column=3)\n",
    "\n",
    "label_perimeter_mean = tk.Label(text='perimeter_mean')\n",
    "label_perimeter_mean.grid(row=1, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "perimeter_mean = tk.Entry()\n",
    "perimeter_mean.grid(row=1, column=5)\n",
    "\n",
    "label_area_mean = tk.Label(text='area_mean')\n",
    "label_area_mean.grid(row=1, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "area_mean = tk.Entry()\n",
    "area_mean.grid(row=1, column=7)\n",
    "\n",
    "label_smoothness_mean = tk.Label(text='smoothness_mean')\n",
    "label_smoothness_mean.grid(row=1, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "smoothness_mean = tk.Entry()\n",
    "smoothness_mean.grid(row=1, column=9)\n",
    "\n",
    "\n",
    "# Columns - second row\n",
    "label_compactness_mean = tk.Label(text='compactness_mean')\n",
    "label_compactness_mean.grid(row=2, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "compactness_mean = tk.Entry()\n",
    "compactness_mean.grid(row=2, column=1)\n",
    "\n",
    "label_concavity_mean = tk.Label(text='concavity_mean')\n",
    "label_concavity_mean.grid(row=2, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concavity_mean = tk.Entry()\n",
    "concavity_mean.grid(row=2, column=3)\n",
    "\n",
    "label_concave_points_mean = tk.Label(text='concave points_mean')\n",
    "label_concave_points_mean.grid(row=2, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concave_points_mean = tk.Entry()\n",
    "concave_points_mean.grid(row=2, column=5)\n",
    "\n",
    "label_symmetry_mean = tk.Label(text='symmetry_mean')\n",
    "label_symmetry_mean.grid(row=2, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "symmetry_mean = tk.Entry()\n",
    "symmetry_mean.grid(row=2, column=7)\n",
    "\n",
    "label_fractal_dimension_mean = tk.Label(text='fractal_dimension_mean')\n",
    "label_fractal_dimension_mean.grid(row=2, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "fractal_dimension_mean = tk.Entry()\n",
    "fractal_dimension_mean.grid(row=2, column=9)\n",
    "\n",
    "\n",
    "# Columns - third row\n",
    "label_radius_se = tk.Label(text='radius_se')\n",
    "label_radius_se.grid(row=3, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "radius_se = tk.Entry()\n",
    "radius_se.grid(row=3, column=1)\n",
    "\n",
    "label_texture_se = tk.Label(text='texture_se')\n",
    "label_texture_se.grid(row=3, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "texture_se = tk.Entry()\n",
    "texture_se.grid(row=3, column=3)\n",
    "\n",
    "label_perimeter_se = tk.Label(text='perimeter_se')\n",
    "label_perimeter_se.grid(row=3, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "perimeter_se = tk.Entry()\n",
    "perimeter_se.grid(row=3, column=5)\n",
    "\n",
    "label_area_se = tk.Label(text='area_se')\n",
    "label_area_se.grid(row=3, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "area_se = tk.Entry()\n",
    "area_se.grid(row=3, column=7)\n",
    "\n",
    "label_smoothness_se = tk.Label(text='smoothness_se')\n",
    "label_smoothness_se.grid(row=3, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "smoothness_se = tk.Entry()\n",
    "smoothness_se.grid(row=3, column=9)\n",
    "\n",
    "\n",
    "# Columns - fourth row\n",
    "label_compactness_se = tk.Label(text='compactness_se')\n",
    "label_compactness_se.grid(row=4, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "compactness_se = tk.Entry()\n",
    "compactness_se.grid(row=4, column=1)\n",
    "\n",
    "label_concavity_se = tk.Label(text='concavity_se')\n",
    "label_concavity_se.grid(row=4, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concavity_se = tk.Entry()\n",
    "concavity_se.grid(row=4, column=3)\n",
    "\n",
    "label_concave_points_se = tk.Label(text='concave points_se')\n",
    "label_concave_points_se.grid(row=4, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concave_points_se = tk.Entry()\n",
    "concave_points_se.grid(row=4, column=5)\n",
    "\n",
    "label_symmetry_se = tk.Label(text='symmetry_se')\n",
    "label_symmetry_se.grid(row=4, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "symmetry_se = tk.Entry()\n",
    "symmetry_se.grid(row=4, column=7)\n",
    "\n",
    "label_fractal_dimension_se = tk.Label(text='fractal_dimension_se')\n",
    "label_fractal_dimension_se.grid(row=4, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "fractal_dimension_se = tk.Entry()\n",
    "fractal_dimension_se.grid(row=4, column=9)\n",
    "\n",
    "\n",
    "# Columns - fifth row\n",
    "label_radius_worst = tk.Label(text='radius_worst')\n",
    "label_radius_worst.grid(row=5, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "radius_worst = tk.Entry()\n",
    "radius_worst.grid(row=5, column=1)\n",
    "\n",
    "label_texture_worst = tk.Label(text='texture_worst')\n",
    "label_texture_worst.grid(row=5, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "texture_worst = tk.Entry()\n",
    "texture_worst.grid(row=5, column=3)\n",
    "\n",
    "label_perimeter_worst = tk.Label(text='perimeter_worst')\n",
    "label_perimeter_worst.grid(row=5, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "perimeter_worst = tk.Entry()\n",
    "perimeter_worst.grid(row=5, column=5)\n",
    "\n",
    "label_area_worst = tk.Label(text='area_worst')\n",
    "label_area_worst.grid(row=5, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "area_worst = tk.Entry()\n",
    "area_worst.grid(row=5, column=7)\n",
    "\n",
    "label_smoothness_worst = tk.Label(text='smoothness_worst')\n",
    "label_smoothness_worst.grid(row=5, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "smoothness_worst = tk.Entry()\n",
    "smoothness_worst.grid(row=5, column=9)\n",
    "\n",
    "\n",
    "# Columns - sixth row\n",
    "label_compactness_worst = tk.Label(text='compactness_worst')\n",
    "label_compactness_worst.grid(row=6, column=0, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "compactness_worst = tk.Entry()\n",
    "compactness_worst.grid(row=6, column=1)\n",
    "\n",
    "label_concavity_worst = tk.Label(text='concavity_worst')\n",
    "label_concavity_worst.grid(row=6, column=2, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concavity_worst = tk.Entry()\n",
    "concavity_worst.grid(row=6, column=3)\n",
    "\n",
    "label_concave_points_worst = tk.Label(text='concave points_worst')\n",
    "label_concave_points_worst.grid(row=6, column=4, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "concave_points_worst = tk.Entry()\n",
    "concave_points_worst.grid(row=6, column=5)\n",
    "\n",
    "label_symmetry_worst = tk.Label(text='symmetry_worst')\n",
    "label_symmetry_worst.grid(row=6, column=6, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "symmetry_worst = tk.Entry()\n",
    "symmetry_worst.grid(row=6, column=7)\n",
    "\n",
    "label_fractal_dimension_worst = tk.Label(text='fractal_dimension_worst')\n",
    "label_fractal_dimension_worst.grid(row=6, column=8, padx=10, pady=10,sticky='nswe', columnspan=1)\n",
    "fractal_dimension_worst = tk.Entry()\n",
    "fractal_dimension_worst.grid(row=6, column=9)\n",
    "\n",
    "# Button\n",
    "font_style_button = (\"Arial\", 14)\n",
    "button_predict = tk.Button(text='Perform analysis', command=breast_cancer_predict, bg='#658bd2', font=font_style_button, fg='white')\n",
    "button_predict.grid(row=7, column=3, padx=10, pady=10, sticky='nswe', columnspan=2)\n",
    "\n",
    "# Show result\n",
    "label_button = tk.Label(text='Result: N/A', borderwidth=1, relief='solid', font=font_style_button)\n",
    "label_button.grid(row=7, column=5, padx=10, pady=10, sticky='nswe', columnspan=2)\n",
    "\n",
    "program.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d17c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
